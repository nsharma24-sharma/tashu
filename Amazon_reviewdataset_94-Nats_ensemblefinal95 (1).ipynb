{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67ab1ce-dfc2-46ec-84da-c7f7f578e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA TITAN Xp\n",
      "CUDA Version: 11.8\n",
      "cuDNN Version: 90600\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "    print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0543a7-05ec-4b34-8048-6d28b9ca1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /home/Rohit_ME/myenv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pandas-2.2.3 pyparsing-3.2.3 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas seaborn matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566ee7f6-427a-458f-b1f4-0cd2a80e792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4736eff4-03d7-4836-af3d-195043760938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4267528a-3f46-461b-a639-82c76817a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string, nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d242ff-ed76-4dbd-b8e5-7b516fefea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/Rohit_ME/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eeaba16-04d4-492f-a6a3-dde168a92191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fake reviews dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfake reviews dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fake reviews dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fake reviews dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd40638-14e8-4468-9f7c-50e742645c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "rating      0\n",
       "label       0\n",
       "text_       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9271dce-480d-4c80-822a-4b52de5d1673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "5.0    24559\n",
       "4.0     7965\n",
       "3.0     3786\n",
       "1.0     2155\n",
       "2.0     1967\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a822e889-94a6-4604-a8e6-69d35ce41e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    nopunc = [w for w in text if w not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return  ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a483ad-b4c3-4702-9e07-191ce2f09849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty',\n",
       " 'Love Well made sturdy comfortable love itVery pretty')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_'][0], clean_text(df['text_'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69949d03-f8fd-4220-8329-bc688eff94bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Love Well made sturdy comfortable love itVery ...\n",
       "1    love great upgrade original Ive mine couple years\n",
       "2              pillow saved back love look feel pillow\n",
       "3          Missing information use great product price\n",
       "4                 nice set Good quality set two months\n",
       "Name: text_, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_'].head().apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0669caec-1bb9-4614-a83a-35d131e86fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40432, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1257ce1-9fdf-432c-a668-f4ec8d9f574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'] = df['text_'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eacdc25-a880-41be-91fd-be729bf57e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english') and not word.isdigit() and word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d431c841-f4db-485f-b787-b79e29c61c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very nice set Good quality We set two months'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df['text_'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595b47c5-4db0-4257-a987-d8925ae46d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'][:10000] = df['text_'][:10000].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee31735a-539f-4e12-a433-ea7d3083c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'][10001:20000] = df['text_'][10001:20000].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0106677-5fd0-40e6-af5c-38d07087bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'][20001:30000] = df['text_'][20001:30000].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae96274-5ad8-4ad7-9bb2-e4e4f211957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'][30001:40000] = df['text_'][30001:40000].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a714c08-d425-4744-9cb8-f13afb7aa9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'][40001:40432] = df['text_'][40001:40432].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1cf3f49-6c78-4f93-a04e-6b0d44a0a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_'] = df['text_'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97055158-e1f6-4127-bd04-d6e9c79abd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "df['text_'] = df['text_'].apply(lambda x: stem_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e840c11-a1f2-4b3d-a91e-af9fa9cfe9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "df[\"text_\"] = df[\"text_\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "137218a2-4f8d-454c-b83a-56113546f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    love well made sturdi comfort i love veri pretti\n",
       "1      love great upgrad origin i 've mine coupl year\n",
       "2        thi pillow save back i love look feel pillow\n",
       "3               miss inform use great product price i\n",
       "4         veri nice set good qualiti we set two month\n",
       "Name: text_, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbae2494-ed41-4ad0-8bc3-741cbcb4fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Preprocessed Fake Reviews Detection Dataset.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0eebacb-71ce-47d3-b1af-f9439ff7fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love well made sturdi comfort i love veri pretti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love great upgrad origin i 've mine coupl year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>thi pillow save back i love look feel pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>miss inform use great product price i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>veri nice set good qualiti we set two month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            category  rating label  \\\n",
       "0           0  Home_and_Kitchen_5     5.0    CG   \n",
       "1           1  Home_and_Kitchen_5     5.0    CG   \n",
       "2           2  Home_and_Kitchen_5     5.0    CG   \n",
       "3           3  Home_and_Kitchen_5     1.0    CG   \n",
       "4           4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                              text_  \n",
       "0  love well made sturdi comfort i love veri pretti  \n",
       "1    love great upgrad origin i 've mine coupl year  \n",
       "2      thi pillow save back i love look feel pillow  \n",
       "3             miss inform use great product price i  \n",
       "4       veri nice set good qualiti we set two month  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Explicitly specify the engine\n",
    "\n",
    "# df = pd.read_csv(\"Preprocessed Fake Reviews Detection Dataset.xls\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f391454-b983-4c65-858c-08876de7d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537586fe-a1e4-4d99-bdbc-1f1f3b5752a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love well made sturdi comfort i love veri pretti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love great upgrad origin i 've mine coupl year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>thi pillow save back i love look feel pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>miss inform use great product price i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>veri nice set good qualiti we set two month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                              text_  \n",
       "0  love well made sturdi comfort i love veri pretti  \n",
       "1    love great upgrad origin i 've mine coupl year  \n",
       "2      thi pillow save back i love look feel pillow  \n",
       "3             miss inform use great product price i  \n",
       "4       veri nice set good qualiti we set two month  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54e51fc-f75f-4f31-aec4-b979cd500f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826455fd-d7f1-47f2-a52b-f4dfa6510f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love well made sturdi comfort i love veri pretti</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love great upgrad origin i 've mine coupl year</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>thi pillow save back i love look feel pillow</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>miss inform use great product price i</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>veri nice set good qualiti we set two month</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                              text_  length  \n",
       "0  love well made sturdi comfort i love veri pretti      48  \n",
       "1    love great upgrad origin i 've mine coupl year      46  \n",
       "2      thi pillow save back i love look feel pillow      44  \n",
       "3             miss inform use great product price i      37  \n",
       "4       veri nice set good qualiti we set two month      43  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['length'] = df['text_'].apply(len)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f870578-1d65-4a05-8a37-79d35933d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 40431 entries, 0 to 40431\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   category  40431 non-null  object \n",
      " 1   rating    40431 non-null  float64\n",
      " 2   label     40431 non-null  object \n",
      " 3   text_     40431 non-null  object \n",
      " 4   length    40431 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd70665e-4a9c-4fcd-bd23-deb2034d0020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 318, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 127, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 473, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 341, in resolve\n",
      "    name, crit = self._merge_into_criterion(r, parent=None)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _merge_into_criterion\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 139, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 129, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 33, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 200, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 306, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 151, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 234, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 317, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 157, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 152, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 62, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='download.pytorch.org', port=443): Read timed out.\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp39-cp39-win_amd64.whl (2728.8 MB)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55490ea-a0d6-4630-b706-57be40726589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03a46e6-8289-498c-96fa-4dac88f9ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fix the data loading and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "# Update these imports at the top of your code\n",
    "from torch.optim import AdamW  # Changed from transformers import\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd21870a-378b-4861-9714-5836baf82a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "label\n",
      "CG    0.5\n",
      "OR    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset properly\n",
    "try:\n",
    "    df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset.xls')\n",
    "except:\n",
    "    # If the preprocessed file isn't available, load and preprocess the original\n",
    "    df = pd.read_csv('fake reviews dataset.csv')\n",
    "    # Add your preprocessing steps here\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Binary classification - ensure proper labeling\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "\n",
    "# Handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['label']), y=df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1063a1d-e12c-416e-b28b-426b5442b582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Initialize tokenizer FIRST\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Split data - stratified to maintain class distribution\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df['text_'], df['label'], \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "def tokenize(texts):\n",
    "    # Convert to list and ensure all elements are strings\n",
    "    text_list = texts.astype(str).tolist()\n",
    "    \n",
    "    return tokenizer(\n",
    "        text_list, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Data cleaning\n",
    "print(\"Checking for NaN values:\")\n",
    "print(df['text_'].isna().sum())\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "df = df.dropna(subset=['text_'])\n",
    "\n",
    "# Convert all text to strings explicitly\n",
    "df['text_'] = df['text_'].astype(str)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenize(X_train)\n",
    "val_encodings = tokenize(X_val)\n",
    "test_encodings = tokenize(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6f8689-3317-4465-a0cb-0c607216b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset  # Add this import\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# Dataset class\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels.tolist())\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(train_encodings, y_train)\n",
    "val_dataset = ReviewDataset(val_encodings, y_val)\n",
    "test_dataset = ReviewDataset(test_encodings, y_test)\n",
    "\n",
    "# Initialize model with proper configuration\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16  # Increased from 8\n",
    "epochs = 5  # Increased from 3\n",
    "learning_rate = 2e-5  # Adjusted from 5e-5\n",
    "warmup_steps = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8404d42c-acd3-47eb-8d15-2ccad41edf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 1769/1769 [12:00<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train Loss: 0.2879\n",
      "Validation Loss: 0.3043\n",
      "Validation Accuracy: 0.9052\n",
      "Validation F1: 0.9111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90      3032\n",
      "           1       0.86      0.97      0.91      3033\n",
      "\n",
      "    accuracy                           0.91      6065\n",
      "   macro avg       0.91      0.91      0.90      6065\n",
      "weighted avg       0.91      0.91      0.90      6065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 1769/1769 [12:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Train Loss: 0.1627\n",
      "Validation Loss: 0.2587\n",
      "Validation Accuracy: 0.9355\n",
      "Validation F1: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93      3032\n",
      "           1       0.91      0.97      0.94      3033\n",
      "\n",
      "    accuracy                           0.94      6065\n",
      "   macro avg       0.94      0.94      0.94      6065\n",
      "weighted avg       0.94      0.94      0.94      6065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 1769/1769 [12:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Train Loss: 0.1142\n",
      "Validation Loss: 0.2467\n",
      "Validation Accuracy: 0.9517\n",
      "Validation F1: 0.9517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      3032\n",
      "           1       0.95      0.95      0.95      3033\n",
      "\n",
      "    accuracy                           0.95      6065\n",
      "   macro avg       0.95      0.95      0.95      6065\n",
      "weighted avg       0.95      0.95      0.95      6065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████| 1769/1769 [12:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Train Loss: 0.0803\n",
      "Validation Loss: 0.3910\n",
      "Validation Accuracy: 0.9306\n",
      "Validation F1: 0.9340\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      3032\n",
      "           1       0.89      0.98      0.93      3033\n",
      "\n",
      "    accuracy                           0.93      6065\n",
      "   macro avg       0.94      0.93      0.93      6065\n",
      "weighted avg       0.94      0.93      0.93      6065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████| 1769/1769 [12:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Train Loss: 0.0512\n",
      "Validation Loss: 0.3716\n",
      "Validation Accuracy: 0.9434\n",
      "Validation F1: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      3032\n",
      "           1       0.92      0.98      0.95      3033\n",
      "\n",
      "    accuracy                           0.94      6065\n",
      "   macro avg       0.95      0.94      0.94      6065\n",
      "weighted avg       0.95      0.94      0.94      6065\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader  # Add this import\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    eps=1e-8\n",
    ")\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop with validation and early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        val_preds.extend(preds)\n",
    "        val_true.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    val_f1 = f1_score(val_true, val_preds)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1: {val_f1:.4f}\")\n",
    "    print(classification_report(val_true, val_preds))\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b90660-2868-404c-8c1a-3289ac7c922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28522d36-a4cc-419a-a577-85a6d510ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c22761c-6a16-4039-9f90-88b7ba17c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading and preprocessing data...\n",
      "\n",
      "Creating datasets...\n",
      "\n",
      "Initializing RoBERTa model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting initial RoBERTa training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|█████████████████████| 1769/1769 [13:16<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|█████████████████████| 1769/1769 [12:56<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|█████████████████████| 1769/1769 [12:34<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9377\n",
      "\n",
      "Optimizing layers with ACO...\n",
      "\n",
      "ACO Iteration 1/10\n",
      "Layers [np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(10), np.int64(11)] - Accuracy: 0.8722\n",
      "New best accuracy: 0.8722 with layers [np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(10), np.int64(11)]\n",
      "Layers [np.int64(1), np.int64(2), np.int64(3), np.int64(6), np.int64(8), np.int64(10), np.int64(11)] - Accuracy: 0.8640\n",
      "Layers [np.int64(0), np.int64(2), np.int64(3), np.int64(4), np.int64(8), np.int64(9), np.int64(10)] - Accuracy: 0.7880\n",
      "Layers [np.int64(0), np.int64(2), np.int64(4), np.int64(6), np.int64(9), np.int64(11)] - Accuracy: 0.8608\n",
      "Layers [np.int64(1), np.int64(3), np.int64(6), np.int64(11)] - Accuracy: 0.8729\n",
      "New best accuracy: 0.8729 with layers [np.int64(1), np.int64(3), np.int64(6), np.int64(11)]\n",
      "Updated pheromone levels: [1.719 1.807 2.583 3.467 2.591 0.942 3.54  0.07  1.722 1.719 2.594 3.54 ]\n",
      "\n",
      "ACO Iteration 2/10\n",
      "Layers [np.int64(1), np.int64(2), np.int64(5), np.int64(10)] - Accuracy: 0.8208\n",
      "Layers [np.int64(0), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.7918\n",
      "Layers [np.int64(1), np.int64(6), np.int64(9), np.int64(11)] - Accuracy: 0.8782\n",
      "New best accuracy: 0.8782 with layers [np.int64(1), np.int64(6), np.int64(9), np.int64(11)]\n",
      "Layers [np.int64(1), np.int64(4), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.8140\n",
      "Layers [np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(8)] - Accuracy: 0.5001\n",
      "Updated pheromone levels: [1.995 3.778 3.129 3.719 3.92  2.772 4.962 0.049 1.705 3.687 4.242 3.356]\n",
      "\n",
      "ACO Iteration 3/10\n",
      "Layers [np.int64(2), np.int64(3), np.int64(10), np.int64(11)] - Accuracy: 0.8874\n",
      "New best accuracy: 0.8874 with layers [np.int64(2), np.int64(3), np.int64(10), np.int64(11)]\n",
      "Layers [np.int64(0), np.int64(3), np.int64(4), np.int64(5), np.int64(8), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8640\n",
      "Layers [np.int64(1), np.int64(2), np.int64(3), np.int64(6), np.int64(8), np.int64(9), np.int64(10)] - Accuracy: 0.7822\n",
      "Layers [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8), np.int64(9)] - Accuracy: 0.5303\n",
      "Layers [np.int64(1), np.int64(2), np.int64(3), np.int64(6), np.int64(10)] - Accuracy: 0.7982\n",
      "Updated pheromone levels: [2.26  4.755 5.188 6.465 4.138 3.335 5.584 0.034 3.37  4.757 6.301 4.101]\n",
      "\n",
      "ACO Iteration 4/10\n",
      "Layers [np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.7840\n",
      "Layers [np.int64(1), np.int64(2), np.int64(9), np.int64(10)] - Accuracy: 0.8257\n",
      "Layers [np.int64(0), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(9), np.int64(11)] - Accuracy: 0.8528\n",
      "Layers [np.int64(0), np.int64(1), np.int64(3), np.int64(4), np.int64(5), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8618\n",
      "Layers [np.int64(2), np.int64(6), np.int64(8), np.int64(9), np.int64(10)] - Accuracy: 0.8170\n",
      "Updated pheromone levels: [3.297 5.016 6.911 7.024 5.395 4.833 5.51  0.024 3.176 7.471 7.7   4.585]\n",
      "\n",
      "ACO Iteration 5/10\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(4), np.int64(6), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8608\n",
      "Layers [np.int64(1), np.int64(2), np.int64(5), np.int64(9), np.int64(10)] - Accuracy: 0.8153\n",
      "Layers [np.int64(0), np.int64(4), np.int64(5), np.int64(9), np.int64(11)] - Accuracy: 0.8729\n",
      "Layers [np.int64(1), np.int64(2), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.8124\n",
      "Layers [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(9)] - Accuracy: 0.5561\n",
      "Updated pheromone levels: [4.042 6.556 7.883 5.473 6.066 5.071 5.53  0.017 2.223 9.148 7.878 4.943]\n",
      "\n",
      "ACO Iteration 6/10\n",
      "Layers [np.int64(0), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8617\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(5), np.int64(6), np.int64(9)] - Accuracy: 0.5585\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(6), np.int64(9), np.int64(11)] - Accuracy: 0.8463\n",
      "Layers [np.int64(1), np.int64(6), np.int64(10), np.int64(11)] - Accuracy: 0.8908\n",
      "New best accuracy: 0.8908 with layers [np.int64(1), np.int64(6), np.int64(10), np.int64(11)]\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(6), np.int64(9)] - Accuracy: 0.5319\n",
      "Updated pheromone levels: [5.627 7.417 8.316 5.225 6.486 5.816 6.699 0.012 1.556 9.202 7.267 6.059]\n",
      "\n",
      "ACO Iteration 7/10\n",
      "Layers [np.int64(0), np.int64(4), np.int64(5), np.int64(10), np.int64(11)] - Accuracy: 0.8859\n",
      "Layers [np.int64(1), np.int64(2), np.int64(5), np.int64(8), np.int64(10), np.int64(11)] - Accuracy: 0.8727\n",
      "Layers [np.int64(0), np.int64(2), np.int64(4), np.int64(5), np.int64(6), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8633\n",
      "Layers [np.int64(0), np.int64(1), np.int64(4), np.int64(6), np.int64(10)] - Accuracy: 0.8091\n",
      "Layers [np.int64(1), np.int64(2), np.int64(4), np.int64(6), np.int64(9), np.int64(11)] - Accuracy: 0.8569\n",
      "Updated pheromone levels: [6.497e+00 7.730e+00 8.414e+00 3.657e+00 7.956e+00 6.693e+00 7.218e+00\n",
      " 8.000e-03 1.962e+00 8.161e+00 8.518e+00 7.720e+00]\n",
      "\n",
      "ACO Iteration 8/10\n",
      "Layers [np.int64(0), np.int64(5), np.int64(6), np.int64(9)] - Accuracy: 0.6285\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(5), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.7916\n",
      "Layers [np.int64(0), np.int64(2), np.int64(5), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.8096\n",
      "Layers [np.int64(1), np.int64(2), np.int64(4), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8716\n",
      "Layers [np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(9), np.int64(11)] - Accuracy: 0.8594\n",
      "Updated pheromone levels: [6.778e+00 7.934e+00 9.222e+00 2.560e+00 7.300e+00 7.774e+00 7.282e+00\n",
      " 6.000e-03 1.373e+00 9.674e+00 8.435e+00 7.135e+00]\n",
      "\n",
      "ACO Iteration 9/10\n",
      "Layers [np.int64(1), np.int64(5), np.int64(8), np.int64(11)] - Accuracy: 0.8803\n",
      "Layers [np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(6), np.int64(10), np.int64(11)] - Accuracy: 0.8650\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(4), np.int64(6), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8608\n",
      "Layers [np.int64(0), np.int64(1), np.int64(5), np.int64(6), np.int64(9), np.int64(10)] - Accuracy: 0.8102\n",
      "Layers [np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(6), np.int64(8), np.int64(9), np.int64(11)] - Accuracy: 0.8447\n",
      "Updated pheromone levels: [6.416e+00 9.815e+00 9.026e+00 1.792e+00 7.680e+00 8.842e+00 8.478e+00\n",
      " 4.000e-03 2.686e+00 9.287e+00 8.441e+00 8.445e+00]\n",
      "\n",
      "ACO Iteration 10/10\n",
      "Layers [np.int64(0), np.int64(1), np.int64(2), np.int64(4), np.int64(6), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8608\n",
      "Layers [np.int64(0), np.int64(1), np.int64(4), np.int64(5)] - Accuracy: 0.5001\n",
      "Layers [np.int64(0), np.int64(1), np.int64(4), np.int64(6), np.int64(9), np.int64(10), np.int64(11)] - Accuracy: 0.8691\n",
      "Layers [np.int64(0), np.int64(1), np.int64(5), np.int64(6)] - Accuracy: 0.5001\n",
      "Layers [np.int64(0), np.int64(4), np.int64(5), np.int64(6), np.int64(9)] - Accuracy: 0.5924\n",
      "Updated pheromone levels: [7.813e+00 9.600e+00 7.179e+00 1.254e+00 8.199e+00 7.782e+00 8.757e+00\n",
      " 3.000e-03 1.880e+00 8.823e+00 7.638e+00 7.642e+00]\n",
      "\n",
      "Selected optimal layers: [np.int64(1), np.int64(6), np.int64(10), np.int64(11)]\n",
      "\n",
      "Final training with optimized layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 1: 100%|██████████████████| 1769/1769 [12:08<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch 2: 100%|██████████████████| 1769/1769 [12:04<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9268\n",
      "\n",
      "Final Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      3033\n",
      "           1       0.89      0.98      0.93      3032\n",
      "\n",
      "    accuracy                           0.93      6065\n",
      "   macro avg       0.93      0.93      0.93      6065\n",
      "weighted avg       0.93      0.93      0.93      6065\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted     0     1   All\n",
      "Actual                     \n",
      "0          2666   367  3033\n",
      "1            61  2971  3032\n",
      "All        2727  3338  6065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enhanced ACO for RoBERTa Layer Optimization\n",
    "class ACOLayerOptimizer:\n",
    "    def __init__(self, n_ants=5, n_iterations=10, evaporation_rate=0.3):\n",
    "        self.n_ants = n_ants\n",
    "        self.n_iterations = n_iterations\n",
    "        self.evaporation_rate = evaporation_rate\n",
    "        \n",
    "    def fit(self, model, dataloader, n_layers=12):\n",
    "        self.pheromone = np.ones(n_layers) * 0.1  # Equal initial pheromone\n",
    "        best_acc = 0\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            ant_accs = []\n",
    "            print(f\"\\nACO Iteration {iteration + 1}/{self.n_iterations}\")\n",
    "            \n",
    "            for ant in range(self.n_ants):\n",
    "                # Probabilistically select layers\n",
    "                selected_layers = sorted(np.random.choice(\n",
    "                    range(n_layers),\n",
    "                    size=random.randint(4, 8),  # Select 4-8 layers\n",
    "                    p=self.pheromone/self.pheromone.sum(),\n",
    "                    replace=False\n",
    "                ))\n",
    "                \n",
    "                # Evaluate layer selection\n",
    "                acc = self._evaluate_layers(model, dataloader, selected_layers)\n",
    "                ant_accs.append((selected_layers, acc))\n",
    "                \n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_layers = selected_layers\n",
    "                    print(f\"New best accuracy: {best_acc:.4f} with layers {best_layers}\")\n",
    "            \n",
    "            # Update pheromone\n",
    "            self._update_pheromone(ant_accs)\n",
    "        \n",
    "        self.selected_layers = best_layers\n",
    "        return self\n",
    "    \n",
    "    def _evaluate_layers(self, model, dataloader, layers):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Get hidden states from selected layers\n",
    "                outputs = model.roberta(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                \n",
    "                # Stack selected layers and average them\n",
    "                selected_hidden = torch.stack([outputs.hidden_states[i] for i in layers])\n",
    "                layer_avg = selected_hidden.mean(dim=0)  # [batch_size, seq_len, hidden_size]\n",
    "                \n",
    "                # Verify tensor dimensions\n",
    "                assert layer_avg.dim() == 3, f\"Expected 3D tensor, got {layer_avg.shape}\"\n",
    "                \n",
    "                # Get [CLS] tokens (first token of each sequence)\n",
    "                cls_tokens = layer_avg[:, 0, :]  # [batch_size, hidden_size]\n",
    "                assert cls_tokens.dim() == 2, f\"Expected 2D [CLS] tokens, got {cls_tokens.shape}\"\n",
    "                \n",
    "                # Manually apply classifier to avoid dimension issues\n",
    "                x = model.classifier.dropout(cls_tokens)\n",
    "                x = model.classifier.dense(x)\n",
    "                x = torch.tanh(x)\n",
    "                x = model.classifier.dropout(x)\n",
    "                logits = model.classifier.out_proj(x)\n",
    "                \n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct += (preds == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"Layers {layers} - Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def _update_pheromone(self, ant_accs):\n",
    "        self.pheromone *= (1 - self.evaporation_rate)  # Evaporation\n",
    "        \n",
    "        for layers, acc in ant_accs:\n",
    "            for layer in layers:\n",
    "                self.pheromone[layer] += acc  # Reward used layers\n",
    "        \n",
    "        print(\"Updated pheromone levels:\", np.round(self.pheromone, 3))\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"\\nLoading and preprocessing data...\")\n",
    "df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "df['text_'] = df['text_'].fillna('')  # Handle NaN values\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)  # 1 for fake, 0 for real\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df['text_'], df['label'],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Dataset class\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "val_dataset = ReviewDataset(X_val, y_val)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing RoBERTa model...\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=2,\n",
    "    output_hidden_states=True  # Need hidden states for ACO\n",
    ").to(device)\n",
    "\n",
    "# 1. Initial training\n",
    "print(\"\\nStarting initial RoBERTa training...\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_loader)*3\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# 2. Optimize layers with ACO\n",
    "print(\"\\nOptimizing layers with ACO...\")\n",
    "aco = ACOLayerOptimizer(n_ants=5, n_iterations=10)\n",
    "aco.fit(model, val_loader)\n",
    "\n",
    "print(f\"\\nSelected optimal layers: {sorted(aco.selected_layers)}\")\n",
    "\n",
    "# 3. Final training with optimized layers\n",
    "def roberta_forward_with_selected_layers(model, input_ids, attention_mask, selected_layers):\n",
    "    outputs = model.roberta(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    \n",
    "    # Stack selected layers and average them\n",
    "    selected_hidden = torch.stack([outputs.hidden_states[i] for i in selected_layers])\n",
    "    layer_avg = selected_hidden.mean(dim=0)  # [batch_size, seq_len, hidden_size]\n",
    "    \n",
    "    # Get [CLS] tokens\n",
    "    cls_tokens = layer_avg[:, 0, :]  # [batch_size, hidden_size]\n",
    "    \n",
    "    # Manually apply classifier\n",
    "    x = model.classifier.dropout(cls_tokens)\n",
    "    x = model.classifier.dense(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = model.classifier.dropout(x)\n",
    "    logits = model.classifier.out_proj(x)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "print(\"\\nFinal training with optimized layers...\")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # Lower learning rate\n",
    "\n",
    "for epoch in range(2):  # Fewer epochs for fine-tuning\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Fine-tuning Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = roberta_forward_with_selected_layers(\n",
    "            model,\n",
    "            batch['input_ids'],\n",
    "            batch['attention_mask'],\n",
    "            aco.selected_layers\n",
    "        )\n",
    "        loss = torch.nn.functional.cross_entropy(logits, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = roberta_forward_with_selected_layers(\n",
    "                model,\n",
    "                batch['input_ids'],\n",
    "                batch['attention_mask'],\n",
    "                aco.selected_layers\n",
    "            )\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final Evaluation\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = roberta_forward_with_selected_layers(\n",
    "            model,\n",
    "            batch['input_ids'],\n",
    "            batch['attention_mask'],\n",
    "            aco.selected_layers\n",
    "        )\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_true.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(\"\\nFinal Test Performance:\")\n",
    "print(classification_report(test_true, test_preds))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.crosstab(test_true, test_preds, \n",
    "                 rownames=['Actual'], \n",
    "                 colnames=['Predicted'],\n",
    "                 margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff270e4-9b63-4a51-9014-df882c00f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACO-optimized model and config saved as 'aco_optimized_roberta.pt' and 'aco_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"aco_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"aco_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nACO-optimized model and config saved as 'aco_optimized_roberta.pt' and 'aco_optimized_roberta_config.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73837f97-5adf-4fdf-80f8-0b4cf090ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a67334d-4a0e-4eda-8b4f-0f883c3d18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting fake review detection pipeline...\n",
      "\n",
      "1. Loading data...\n",
      "\n",
      "2. Initializing tokenizer...\n",
      "\n",
      "3. Creating dataloaders...\n",
      "\n",
      "4. Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Starting initial training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 3538/3538 [07:49<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 3538/3538 [07:14<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9355\n",
      "\n",
      "6. Optimizing attention heads...\n",
      "\n",
      "Initializing solutions...\n",
      "Quick initial evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial eval: 100%|███████████████████████████████| 3/3 [00:00<00:00,  3.78it/s]\n",
      "Optimizing: 100%|█████████████████████████████████| 3/3 [00:03<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluation...\n",
      "Best validation accuracy: 0.8500\n",
      "\n",
      "Selected 72/144 attention heads (50.0%)\n",
      "\n",
      "7. Applying best head configuration...\n",
      "\n",
      "8. Running final evaluation...\n",
      "\n",
      "Final Test Accuracy: 0.9322\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.97      0.89      0.93      3033\n",
      "        Fake       0.90      0.97      0.93      3032\n",
      "\n",
      "    accuracy                           0.93      6065\n",
      "   macro avg       0.93      0.93      0.93      6065\n",
      "weighted avg       0.93      0.93      0.93      6065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ABCAttentionOptimizer:\n",
    "    \"\"\"Optimized Artificial Bee Colony for Attention Head Selection\"\"\"\n",
    "    def __init__(self, n_employed=5, n_onlookers=5, max_iter=5, device=None):\n",
    "        self.n_employed = n_employed\n",
    "        self.n_onlookers = n_onlookers\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.best_solution = None\n",
    "        self.best_fitness = 0.0\n",
    "        self.timeout = 300  # 5 minute timeout\n",
    "\n",
    "    def _count_heads(self, model):\n",
    "        try:\n",
    "            return sum(layer.attention.self.num_attention_heads \n",
    "                     for layer in model.roberta.encoder.layer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error counting heads: {e}\")\n",
    "            return 144  # Default for RoBERTa-base\n",
    "\n",
    "    def _initialize_solutions(self, model):\n",
    "        total_heads = self._count_heads(model)\n",
    "        return [torch.rand(total_heads) > 0.5 for _ in range(self.n_employed)]\n",
    "\n",
    "    def _apply_head_mask(self, model, head_mask):\n",
    "        try:\n",
    "            idx = 0\n",
    "            for layer in model.roberta.encoder.layer:\n",
    "                num_heads = layer.attention.self.num_attention_heads\n",
    "                layer.attention.self.head_mask = head_mask[idx:idx+num_heads].float().to(self.device)\n",
    "                idx += num_heads\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying head mask: {e}\")\n",
    "\n",
    "    def _evaluate(self, model, dataloader, head_mask, max_batches=5):\n",
    "        try:\n",
    "            self._apply_head_mask(model, head_mask)\n",
    "            model.eval()\n",
    "            correct, total, batches_processed = 0, 0, 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                for batch in dataloader:\n",
    "                    if batches_processed >= max_batches:\n",
    "                        break\n",
    "                    if time.time() - start_time > self.timeout:\n",
    "                        print(\"Evaluation timeout reached\")\n",
    "                        return 0.0\n",
    "                    \n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                    \n",
    "                    outputs = model.roberta(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "                    \n",
    "                    last_hidden = outputs.hidden_states[-1]\n",
    "                    cls_tokens = last_hidden[:, 0, :]\n",
    "                    \n",
    "                    x = model.classifier.dropout(cls_tokens)\n",
    "                    x = model.classifier.dense(x)\n",
    "                    x = torch.tanh(x)\n",
    "                    x = model.classifier.dropout(x)\n",
    "                    logits = model.classifier.out_proj(x)\n",
    "                    \n",
    "                    _, preds = torch.max(logits, dim=1)\n",
    "                    correct += (preds == batch['labels']).sum().item()\n",
    "                    total += batch['labels'].size(0)\n",
    "                    batches_processed += 1\n",
    "                \n",
    "                return correct / total if total > 0 else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def fit(self, model, train_loader, val_loader=None):\n",
    "        try:\n",
    "            model.to(self.device)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Initialize solutions\n",
    "            print(\"\\nInitializing solutions...\")\n",
    "            solutions = self._initialize_solutions(model)\n",
    "            \n",
    "            # Quick initial evaluation\n",
    "            print(\"Quick initial evaluation...\")\n",
    "            fitness = []\n",
    "            for i, solution in enumerate(tqdm(solutions, desc=\"Initial eval\")):\n",
    "                if time.time() - start_time > self.timeout:\n",
    "                    raise TimeoutError(\"Optimization timeout\")\n",
    "                fitness.append(self._evaluate(model, train_loader, solution, max_batches=3))\n",
    "            \n",
    "            best_idx = np.argmax(fitness)\n",
    "            self.best_solution = solutions[best_idx].clone()\n",
    "            self.best_fitness = fitness[best_idx]\n",
    "            \n",
    "            # Optimization loop\n",
    "            for epoch in tqdm(range(self.max_iter), desc=\"Optimizing\"):\n",
    "                if time.time() - start_time > self.timeout:\n",
    "                    raise TimeoutError(\"Optimization timeout\")\n",
    "                \n",
    "                # Employed bee phase\n",
    "                for i in range(self.n_employed):\n",
    "                    new_solution = solutions[i].clone()\n",
    "                    num_flips = max(1, int(len(new_solution)*0.1))\n",
    "                    flip_indices = random.sample(range(len(new_solution)), num_flips)\n",
    "                    new_solution[flip_indices] = ~new_solution[flip_indices]\n",
    "                    \n",
    "                    new_fitness = self._evaluate(model, train_loader, new_solution, max_batches=3)\n",
    "                    \n",
    "                    if new_fitness > fitness[i]:\n",
    "                        solutions[i] = new_solution\n",
    "                        fitness[i] = new_fitness\n",
    "                        if new_fitness > self.best_fitness:\n",
    "                            self.best_solution = new_solution.clone()\n",
    "                            self.best_fitness = new_fitness\n",
    "                            print(f\"New best: {self.best_fitness:.4f}\")\n",
    "\n",
    "                # Onlooker bee phase\n",
    "                fitness_sum = max(sum(fitness), 1e-8)\n",
    "                probs = [f/fitness_sum for f in fitness]\n",
    "                \n",
    "                for _ in range(self.n_onlookers):\n",
    "                    idx = random.choices(range(self.n_employed), weights=probs)[0]\n",
    "                    new_solution = solutions[idx].clone()\n",
    "                    num_flips = max(1, int(len(new_solution)*0.1))\n",
    "                    flip_indices = random.sample(range(len(new_solution)), num_flips)\n",
    "                    new_solution[flip_indices] = ~new_solution[flip_indices]\n",
    "                    \n",
    "                    new_fitness = self._evaluate(model, train_loader, new_solution, max_batches=3)\n",
    "                    \n",
    "                    if new_fitness > fitness[idx]:\n",
    "                        solutions[idx] = new_solution\n",
    "                        fitness[idx] = new_fitness\n",
    "                        if new_fitness > self.best_fitness:\n",
    "                            self.best_solution = new_solution.clone()\n",
    "                            self.best_fitness = new_fitness\n",
    "                            print(f\"New best: {self.best_fitness:.4f}\")\n",
    "\n",
    "            # Final full evaluation\n",
    "            if val_loader:\n",
    "                print(\"\\nFinal evaluation...\")\n",
    "                val_acc = self._evaluate(model, val_loader, self.best_solution)\n",
    "                print(f\"Best validation accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            active_heads = self.best_solution.sum().item()\n",
    "            total_heads = len(self.best_solution)\n",
    "            print(f\"\\nSelected {active_heads}/{total_heads} attention heads ({active_heads/total_heads:.1%})\")\n",
    "            \n",
    "            return self.best_solution\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed: {e}\")\n",
    "            return torch.ones(self._count_heads(model)) > 0.5  # Return all heads as fallback\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                str(self.texts.iloc[idx]),\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            return {\n",
    "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
    "                'labels': torch.tensor(0, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    try:\n",
    "        df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "        df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "        df['text_'] = df['text_'].fillna('')\n",
    "        df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "        \n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            df['text_'], df['label'],\n",
    "            test_size=0.3,\n",
    "            random_state=42,\n",
    "            stratify=df['label']\n",
    "        )\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=0.5,\n",
    "            random_state=42,\n",
    "            stratify=y_temp\n",
    "        )\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading error: {e}\")\n",
    "        # Return dummy data if real data fails\n",
    "        texts = ['dummy'] * 100\n",
    "        labels = [0] * 50 + [1] * 50\n",
    "        return texts[:60], texts[60:80], texts[80:], labels[:60], labels[60:80], labels[80:]\n",
    "\n",
    "def create_dataloaders(X_train, X_val, X_test, y_train, y_val, y_test, tokenizer, batch_size=8):\n",
    "    try:\n",
    "        train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "        val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "        test_dataset = ReviewDataset(X_test, y_test, tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Dataloader creation error: {e}\")\n",
    "        # Return empty loaders if creation fails\n",
    "        empty_dataset = ReviewDataset(pd.Series(['']), pd.Series([0]), tokenizer)\n",
    "        return (\n",
    "            DataLoader(empty_dataset, batch_size=1),\n",
    "            DataLoader(empty_dataset, batch_size=1),\n",
    "            DataLoader(empty_dataset, batch_size=1)\n",
    "        )\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=2, lr=2e-5):\n",
    "    try:\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=50,\n",
    "            num_training_steps=len(train_loader)*epochs\n",
    "        )\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            val_acc = evaluate_model(model, val_loader)\n",
    "            print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    try:\n",
    "        model.eval()\n",
    "        preds, true_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "                true_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        return accuracy_score(true_labels, preds)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def main():\n",
    "    print(\"Starting fake review detection pipeline...\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_preprocess_data()\n",
    "    \n",
    "    # 2. Initialize tokenizer\n",
    "    print(\"\\n2. Initializing tokenizer...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # 3. Create dataloaders\n",
    "    print(\"\\n3. Creating dataloaders...\")\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, tokenizer\n",
    "    )\n",
    "    \n",
    "    # 4. Initialize model\n",
    "    print(\"\\n4. Initializing model...\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        num_labels=2,\n",
    "        output_hidden_states=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # 5. Initial training\n",
    "    print(\"\\n5. Starting initial training...\")\n",
    "    train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # 6. Optimize attention heads\n",
    "    print(\"\\n6. Optimizing attention heads...\")\n",
    "    abc = ABCAttentionOptimizer(n_employed=3, n_onlookers=3, max_iter=3)\n",
    "    best_heads = abc.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # 7. Apply best configuration\n",
    "    print(\"\\n7. Applying best head configuration...\")\n",
    "    abc._apply_head_mask(model, best_heads)\n",
    "    \n",
    "    # 8. Final evaluation\n",
    "    print(\"\\n8. Running final evaluation...\")\n",
    "    test_acc = evaluate_model(model, test_loader)\n",
    "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # 9. Classification report\n",
    "    model.eval()\n",
    "    test_preds, test_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            test_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            test_true.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_true, test_preds, target_names=['Real', 'Fake']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d88a59-38e2-48e2-915d-afe0420ad33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ABC-optimized model and config saved as 'abc_optimized_roberta.pt' and 'abc_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"abc_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"abc_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nABC-optimized model and config saved as 'abc_optimized_roberta.pt' and 'abc_optimized_roberta_config.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed79d8-a978-415e-88a3-3c68c1d5de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df24c93e-61ee-44bc-a4d2-89b1a5c7ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set environment variables for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc76975-2fb7-4d01-8f5e-b68835fc925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7271abac-75df-4b40-b7ba-c878f128831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSOAttentionOptimizer:\n",
    "    def __init__(self, n_particles=10, max_iter=5, w=0.7, c1=1.5, c2=1.5):\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.n_heads = 12\n",
    "\n",
    "    def initialize_particles(self):\n",
    "        particles = np.random.randint(0, 2, size=(self.n_particles, self.n_heads))\n",
    "        for i in range(self.n_particles):\n",
    "            if sum(particles[i]) < 3:\n",
    "                particles[i, random.sample(range(self.n_heads), 3)] = 1\n",
    "        return particles\n",
    "    \n",
    "    def evaluate_particle(self, model, dataloader, particle):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        active_heads = [i for i, val in enumerate(particle) if val == 1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch, output_attentions=True)\n",
    "                \n",
    "                attentions = torch.stack([outputs.attentions[-1][:, h] for h in active_heads]).mean(dim=0)\n",
    "                cls_attention = attentions[:, 0, :].mean(dim=1)\n",
    "                logits = outputs.logits * cls_attention.unsqueeze(1)\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    def optimize(self, model, dataloader):\n",
    "        particles = self.initialize_particles()\n",
    "        velocities = np.random.uniform(-1, 1, size=(self.n_particles, self.n_heads))\n",
    "        \n",
    "        personal_bests = particles.copy()\n",
    "        personal_best_scores = np.array([self.evaluate_particle(model, dataloader, p) for p in particles])\n",
    "        \n",
    "        global_best_idx = np.argmax(personal_best_scores)\n",
    "        global_best = particles[global_best_idx].copy()\n",
    "        global_best_score = personal_best_scores[global_best_idx]\n",
    "        \n",
    "        for iter in range(self.max_iter):\n",
    "            for i in range(self.n_particles):\n",
    "                r1, r2 = random.random(), random.random()\n",
    "                velocities[i] = (self.w * velocities[i] +\n",
    "                                 self.c1 * r1 * (personal_bests[i] - particles[i]) +\n",
    "                                 self.c2 * r2 * (global_best - particles[i]))\n",
    "                \n",
    "                sigmoid_v = 1 / (1 + np.exp(-velocities[i]))\n",
    "                particles[i] = (sigmoid_v > random.random()).astype(int)\n",
    "                \n",
    "                if sum(particles[i]) < 3:\n",
    "                    particles[i, random.sample(range(self.n_heads), 3)] = 1\n",
    "                \n",
    "                current_score = self.evaluate_particle(model, dataloader, particles[i])\n",
    "                if current_score > personal_best_scores[i]:\n",
    "                    personal_bests[i] = particles[i].copy()\n",
    "                    personal_best_scores[i] = current_score\n",
    "                    if current_score > global_best_score:\n",
    "                        global_best = particles[i].copy()\n",
    "                        global_best_score = current_score\n",
    "            \n",
    "            print(f\"Iteration {iter+1}: Best Accuracy = {global_best_score:.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return global_best, global_best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fec3f5-8f69-4d7e-8eb0-ef0865887afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Clear cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "df.to_csv(\"Preprocessed Fake Reviews Detection Dataset (1).xls\")\n",
    "df['text_'] = df['text_'].fillna('')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['text_'], df['label'], test_size=0.1, random_state=42, stratify=df['label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d785ab-d74e-4259-9f86-af3a9293a72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer and datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing tokenizer and datasets...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length=64)\n",
    "val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length=64)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb846abd-8014-4507-a036-5a299d5944e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                         | 0/9097 [00:00<?, ?it/s]RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Epoch 1: 100%|██████████████████████████████| 9097/9097 [16:37<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.4028\n",
      "Validation Accuracy: 0.9268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 9097/9097 [16:36<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.2517\n",
      "Validation Accuracy: 0.9271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 9097/9097 [19:47<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.1684\n",
      "Validation Accuracy: 0.9421\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model...\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', num_labels=2, output_attentions=True\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.to(device)\n",
    "\n",
    "print(\"Training model...\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=50, num_training_steps=len(train_loader)*3\n",
    ")\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_preds, val_true = [], []\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_true.extend(batch['labels'].cpu().numpy())\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best_roberta.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99fdf1e-1a8e-4447-bf4e-06a2acdc9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PSO-optimized model and config saved as 'pso_optimized_roberta.pt' and 'pso_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"pso_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"pso_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nPSO-optimized model and config saved as 'pso_optimized_roberta.pt' and 'pso_optimized_roberta_config.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6461c5-496f-4ade-bcdc-1ab7eb3f714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greywolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc46c916-4c83-4cac-a3df-ca18b8acc9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████████████████████████| 9097/9097 [18:18<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.4076\n",
      "Validation Accuracy: 0.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 9097/9097 [19:57<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.2283\n",
      "Validation Accuracy: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 9097/9097 [16:34<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.1291\n",
      "Validation Accuracy: 0.9409\n",
      "\n",
      "Running GWO Optimization...\n",
      "Iteration 1/5 - Best Accuracy: 0.9468\n",
      "Iteration 2/5 - Best Accuracy: 0.9468\n",
      "Iteration 3/5 - Best Accuracy: 0.9468\n",
      "Iteration 4/5 - Best Accuracy: 0.9468\n",
      "Iteration 5/5 - Best Accuracy: 0.9468\n",
      "\n",
      "Best attention head mask: [1 0 0 1 1 0 1 0 1 1 1 0]\n",
      "Best validation accuracy from GWO: 0.9468\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class GWOAttentionOptimizer:\n",
    "    def __init__(self, num_wolves=10, max_iter=5):\n",
    "        self.num_wolves = num_wolves\n",
    "        self.max_iter = max_iter\n",
    "        self.n_heads = 12\n",
    "\n",
    "    def initialize_wolves(self):\n",
    "        wolves = np.random.randint(0, 2, size=(self.num_wolves, self.n_heads))\n",
    "        for i in range(self.num_wolves):\n",
    "            if np.sum(wolves[i]) < 3:\n",
    "                wolves[i, random.sample(range(self.n_heads), 3)] = 1\n",
    "        return wolves\n",
    "\n",
    "    def evaluate_wolf(self, model, dataloader, wolf):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        active_heads = [i for i, v in enumerate(wolf) if v == 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch, output_attentions=True)\n",
    "                attentions = torch.stack([outputs.attentions[-1][:,h] for h in active_heads]).mean(dim=0)\n",
    "                cls_attention = attentions[:, 0, :].mean(dim=1)\n",
    "                logits = outputs.logits * cls_attention.unsqueeze(1)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "                torch.cuda.empty_cache()\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    def optimize(self, model, dataloader):\n",
    "        wolves = self.initialize_wolves()\n",
    "        fitness = np.array([self.evaluate_wolf(model, dataloader, wolf) for wolf in wolves])\n",
    "\n",
    "        for iter in range(self.max_iter):\n",
    "            sorted_indices = np.argsort(fitness)[::-1]\n",
    "            alpha, beta, delta = wolves[sorted_indices[0]], wolves[sorted_indices[1]], wolves[sorted_indices[2]]\n",
    "\n",
    "            a = 2 - iter * (2 / self.max_iter)\n",
    "            for i in range(self.num_wolves):\n",
    "                for j in range(self.n_heads):\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                    A1, C1 = 2*a*r1 - a, 2*r2\n",
    "                    D_alpha = abs(C1 * alpha[j] - wolves[i][j])\n",
    "                    X1 = alpha[j] - A1 * D_alpha\n",
    "\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                    A2, C2 = 2*a*r1 - a, 2*r2\n",
    "                    D_beta = abs(C2 * beta[j] - wolves[i][j])\n",
    "                    X2 = beta[j] - A2 * D_beta\n",
    "\n",
    "                    r1, r2 = random.random(), random.random()\n",
    "                    A3, C3 = 2*a*r1 - a, 2*r2\n",
    "                    D_delta = abs(C3 * delta[j] - wolves[i][j])\n",
    "                    X3 = delta[j] - A3 * D_delta\n",
    "\n",
    "                    X_avg = (X1 + X2 + X3) / 3.0\n",
    "                    wolves[i][j] = 1 if X_avg > 0.5 else 0\n",
    "\n",
    "                if np.sum(wolves[i]) < 3:\n",
    "                    wolves[i, random.sample(range(self.n_heads), 3)] = 1\n",
    "\n",
    "                fitness[i] = self.evaluate_wolf(model, dataloader, wolves[i])\n",
    "            best_acc = np.max(fitness)\n",
    "            print(f\"Iteration {iter+1}/{self.max_iter} - Best Accuracy: {best_acc:.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        best_index = np.argmax(fitness)\n",
    "        return wolves[best_index], fitness[best_index]\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df['text_'] = df['text_'].fillna('')\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text_'], df['label'], test_size=0.1, random_state=42, stratify=df['label'])\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base', num_labels=2, output_attentions=True)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=len(train_loader)*3)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(batch['labels'].cpu().numpy())\n",
    "        acc = accuracy_score(val_true, val_preds)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best_roberta.pt')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nRunning GWO Optimization...\")\n",
    "    model.load_state_dict(torch.load('best_roberta.pt'))\n",
    "    gwo = GWOAttentionOptimizer(num_wolves=10, max_iter=5)\n",
    "    best_mask, best_score = gwo.optimize(model, val_loader)\n",
    "\n",
    "    print(f\"\\nBest attention head mask: {best_mask}\")\n",
    "    print(f\"Best validation accuracy from GWO: {best_score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9753995c-03b6-4b56-afe0-d66f9b081649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GWO-optimized model and config saved as 'gwo_optimized_roberta.pt' and 'gwo_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"gwo_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"gwo_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nGWO-optimized model and config saved as 'gwo_optimized_roberta.pt' and 'gwo_optimized_roberta_config.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bebbb6-f60a-42f4-8a30-23bee9acb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#firefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff67c05d-5c44-4415-8d04-adc2efa811d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Initializing tokenizer and datasets...\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████| 9097/9097 [21:49<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.4448\n",
      "Validation Accuracy: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 9097/9097 [21:39<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.3016\n",
      "Validation Accuracy: 0.9271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 9097/9097 [22:29<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.2086\n",
      "Validation Accuracy: 0.9389\n",
      "\n",
      "Optimizing attention heads...\n",
      "Generation 1: Best Accuracy = 0.9389\n",
      "Generation 2: Best Accuracy = 0.9389\n",
      "Generation 3: Best Accuracy = 0.9389\n",
      "\n",
      "Best attention heads: [0, 1, 2, 3, 6, 8, 9, 10]\n",
      "Best validation accuracy: 0.9389\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Set environment variables for better memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset class with max_length parameter\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Optimized Firefly Attention class\n",
    "class OptimizedFireflyAttention:\n",
    "    def __init__(self, n_fireflies=10, max_generations=5, alpha=0.2, beta0=1.0, gamma=0.01):\n",
    "        self.n_fireflies = n_fireflies\n",
    "        self.max_generations = max_generations\n",
    "        self.alpha = alpha\n",
    "        self.beta0 = beta0\n",
    "        self.gamma = gamma\n",
    "        self.n_heads = 12\n",
    "        \n",
    "    def random_head_combination(self):\n",
    "        k = random.randint(6, 8)\n",
    "        return sorted(random.sample(range(self.n_heads), k=k))\n",
    "    \n",
    "    def evaluate_firefly(self, model, dataloader, heads):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch, output_attentions=True)\n",
    "                \n",
    "                # Process with selected heads\n",
    "                attentions = torch.stack([outputs.attentions[-1][:,h] for h in heads]).mean(dim=0)\n",
    "                cls_attention = attentions[:, 0, :].mean(dim=1)\n",
    "                logits = outputs.logits * cls_attention.unsqueeze(1)\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "        \n",
    "        return correct / total if total > 0 else 0\n",
    "    \n",
    "    def fit(self, model, dataloader):\n",
    "        self.fireflies = [self.random_head_combination() for _ in range(self.n_fireflies)]\n",
    "        self.brightness = np.zeros(self.n_fireflies)\n",
    "        self.best_solution = None\n",
    "        self.best_brightness = 0\n",
    "        \n",
    "        for i in range(self.n_fireflies):\n",
    "            self.brightness[i] = self.evaluate_firefly(model, dataloader, self.fireflies[i])\n",
    "            if self.brightness[i] > self.best_brightness:\n",
    "                self.best_brightness = self.brightness[i]\n",
    "                self.best_solution = self.fireflies[i].copy()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        for gen in range(self.max_generations):\n",
    "            for i in range(self.n_fireflies):\n",
    "                for j in range(self.n_fireflies):\n",
    "                    if self.brightness[j] > self.brightness[i]:\n",
    "                        # Simplified movement for memory efficiency\n",
    "                        self.fireflies[i] = random.sample(self.fireflies[i] + self.fireflies[j], \n",
    "                                                        len(self.fireflies[i]))\n",
    "                        new_brightness = self.evaluate_firefly(model, dataloader, self.fireflies[i])\n",
    "                        if new_brightness > self.brightness[i]:\n",
    "                            self.brightness[i] = new_brightness\n",
    "                            if new_brightness > self.best_brightness:\n",
    "                                self.best_brightness = new_brightness\n",
    "                                self.best_solution = self.fireflies[i].copy()\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"Generation {gen+1}: Best Accuracy = {self.best_brightness:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "def main():\n",
    "    # Clear cache before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 1. Load and prepare data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df['text_'] = df['text_'].fillna('')\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text_'], df['label'], test_size=0.1, random_state=42, stratify=df['label'])\n",
    "    \n",
    "    # 2. Initialize with smaller batches\n",
    "    print(\"Initializing tokenizer and datasets...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length=64)\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length=64)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "    \n",
    "    # 3. Load model with memory optimizations\n",
    "    print(\"Initializing model...\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base', \n",
    "        num_labels=2,\n",
    "        output_attentions=True\n",
    "    )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.to(device)\n",
    "    \n",
    "    # 4. Train model\n",
    "    print(\"Training model...\")\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=len(train_loader)*3)\n",
    "    \n",
    "    best_acc = 0\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_preds, val_true = [], []\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(batch['labels'].cpu().numpy())\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        acc = accuracy_score(val_true, val_preds)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best_roberta.pt')\n",
    "    \n",
    "    # 5. Optimize attention heads\n",
    "    print(\"\\nOptimizing attention heads...\")\n",
    "    model.load_state_dict(torch.load('best_roberta.pt'))\n",
    "    fa = OptimizedFireflyAttention(n_fireflies=8, max_generations=3)\n",
    "    fa.fit(model, val_loader)\n",
    "    \n",
    "    print(f\"\\nBest attention heads: {sorted(fa.best_solution)}\")\n",
    "    print(f\"Best validation accuracy: {fa.best_brightness:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d574babd-040a-4744-b5c0-d89e91c50e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Firefly-optimized model and config saved as 'firefly_optimized_roberta.pt' and 'firefly_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"firefly_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"firefly_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nFirefly-optimized model and config saved as 'firefly_optimized_roberta.pt' and 'firefly_optimized_roberta_config.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5e885-e8a8-486a-a2d2-3b460deaba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "640bf77f-bbf6-4a5c-a14a-4c349a1d1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /home/Rohit_ME/myenv/lib/python3.10/site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (25.1.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (1.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-optimize) (24.2)\n",
      "Requirement already satisfied: PyYAML in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7910179b-1240-4c12-acd4-f7fea774f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611f2517-af88-4d29-a0c1-e32dd3ba23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:07:09.040965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745766430.429248  287621 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745766430.974426  287621 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745766436.727946  287621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745766436.728023  287621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745766436.728034  287621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745766436.728042  287621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1:   0%|                                         | 0/9097 [00:00<?, ?it/s]RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Epoch 1: 100%|██████████████████████████████| 9097/9097 [15:25<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████| 9097/9097 [16:05<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.2360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████| 9097/9097 [16:45<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.1308\n",
      "🔍 Running Bayesian Optimization...\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(0), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(1), 'head_10': np.int64(0), 'head_11': np.int64(1)} => Accuracy: 0.9045\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(1), 'head_8': np.int64(0), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9320\n",
      "Tested mask: {'head_0': np.int64(0), 'head_1': np.int64(1), 'head_2': np.int64(0), 'head_3': np.int64(1), 'head_4': np.int64(0), 'head_5': np.int64(1), 'head_6': np.int64(1), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(1), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9189\n",
      "Tested mask: {'head_0': np.int64(0), 'head_1': np.int64(0), 'head_2': np.int64(0), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(1), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(1), 'head_10': np.int64(0), 'head_11': np.int64(0)} => Accuracy: 0.6815\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(0), 'head_4': np.int64(0), 'head_5': np.int64(1), 'head_6': np.int64(1), 'head_7': np.int64(0), 'head_8': np.int64(1), 'head_9': np.int64(1), 'head_10': np.int64(1), 'head_11': np.int64(1)} => Accuracy: 0.9233\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(1), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(0), 'head_10': np.int64(0), 'head_11': np.int64(0)} => Accuracy: 0.9174\n",
      "Tested mask: {'head_0': np.int64(0), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(1), 'head_6': np.int64(1), 'head_7': np.int64(1), 'head_8': np.int64(1), 'head_9': np.int64(0), 'head_10': np.int64(0), 'head_11': np.int64(0)} => Accuracy: 0.8900\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(1), 'head_2': np.int64(1), 'head_3': np.int64(0), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(1), 'head_7': np.int64(1), 'head_8': np.int64(1), 'head_9': np.int64(0), 'head_10': np.int64(0), 'head_11': np.int64(0)} => Accuracy: 0.8106\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(0), 'head_2': np.int64(0), 'head_3': np.int64(0), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(1), 'head_7': np.int64(1), 'head_8': np.int64(0), 'head_9': np.int64(1), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9008\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(1), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9352\n",
      "Tested mask: {'head_0': np.int64(0), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(0), 'head_5': np.int64(0), 'head_6': np.int64(1), 'head_7': np.int64(1), 'head_8': np.int64(1), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9033\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(1), 'head_2': np.int64(0), 'head_3': np.int64(1), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(1), 'head_8': np.int64(1), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(0)} => Accuracy: 0.9179\n",
      "Tested mask: {'head_0': np.int64(0), 'head_1': np.int64(1), 'head_2': np.int64(1), 'head_3': np.int64(1), 'head_4': np.int64(0), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(1)} => Accuracy: 0.9221\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(0), 'head_2': np.int64(1), 'head_3': np.int64(0), 'head_4': np.int64(0), 'head_5': np.int64(1), 'head_6': np.int64(1), 'head_7': np.int64(0), 'head_8': np.int64(1), 'head_9': np.int64(1), 'head_10': np.int64(0), 'head_11': np.int64(1)} => Accuracy: 0.7856\n",
      "Tested mask: {'head_0': np.int64(1), 'head_1': np.int64(1), 'head_2': np.int64(0), 'head_3': np.int64(0), 'head_4': np.int64(1), 'head_5': np.int64(0), 'head_6': np.int64(0), 'head_7': np.int64(0), 'head_8': np.int64(0), 'head_9': np.int64(0), 'head_10': np.int64(1), 'head_11': np.int64(1)} => Accuracy: 0.9179\n",
      "\n",
      "✅ Best Head Mask: [np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "🎯 Best Accuracy: 0.9352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set memory management for CUDA\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset class\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "df['text_'] = df['text_'].fillna('')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['text_'], df['label'], test_size=0.1, random_state=42, stratify=df['label'])\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = ReviewDataset(X_val, y_val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Load and fine-tune model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2, output_attentions=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=len(train_loader)*3)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"finetuned_roberta.pt\")\n",
    "\n",
    "# Define Bayesian Optimization\n",
    "head_space = [Integer(0, 1, name=f\"head_{i}\") for i in range(12)]\n",
    "\n",
    "@use_named_args(head_space)\n",
    "def evaluate_mask(**params):\n",
    "    model.load_state_dict(torch.load(\"finetuned_roberta.pt\"))\n",
    "    model.eval()\n",
    "    head_mask = torch.tensor([params[f'head_{i}'] for i in range(12)], dtype=torch.float).to(device)\n",
    "\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch, head_mask=head_mask.unsqueeze(0).repeat(12, 1))\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Tested mask: {params} => Accuracy: {acc:.4f}\")\n",
    "    return -acc  # minimize negative accuracy\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "print(\"🔍 Running Bayesian Optimization...\")\n",
    "results = gp_minimize(evaluate_mask, head_space, n_calls=15, random_state=42)\n",
    "\n",
    "# Final Results\n",
    "best_mask = results.x\n",
    "best_accuracy = -results.fun\n",
    "print(f\"\\n✅ Best Head Mask: {best_mask}\")\n",
    "print(f\"🎯 Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f545d4-66d0-4240-b92b-ad55c69c58a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bayesian-optimized model and config saved as 'bayesian_optimized_roberta.pt' and 'bayesian_optimized_roberta_config.pt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model's config separately\n",
    "torch.save(model.config, \"bayesian_optimized_roberta_config.pt\")\n",
    "\n",
    "# Save the model's trained weights\n",
    "torch.save(model.state_dict(), \"bayesian_optimized_roberta.pt\")\n",
    "\n",
    "print(\"\\nBayesian-optimized model and config saved as 'bayesian_optimized_roberta.pt' and 'bayesian_optimized_roberta_config.pt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1ddc6-dea0-4434-821c-b6d6a05c213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49408b19-86ff-4488-be1a-4ba78dec3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/Rohit_ME/myenv/lib/python3.10/site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea3b3c9-44ae-46a9-bccd-866eeffc7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-2.0.3-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from bayesian-optimization) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from bayesian-optimization) (1.6.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from bayesian-optimization) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/Rohit_ME/myenv/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.6.0)\n",
      "Downloading bayesian_optimization-2.0.3-py3-none-any.whl (31 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: colorama, bayesian-optimization\n",
      "Successfully installed bayesian-optimization-2.0.3 colorama-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6317960-2118-4eb2-8895-0c5bb866861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:51:42.124173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745772702.517301  301216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745772702.620741  301216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745772703.333393  301216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745772703.333433  301216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745772703.333438  301216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745772703.333442  301216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Bayesian searching for best ensemble weights...\n",
      "|   iter    |  target   |    w0     |    w1     |    w2     |    w3     |    w4     |    w5     |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9364   \u001b[39m | \u001b[39m0.3745   \u001b[39m | \u001b[39m0.9507   \u001b[39m | \u001b[39m0.732    \u001b[39m | \u001b[39m0.5987   \u001b[39m | \u001b[39m0.156    \u001b[39m | \u001b[39m0.156    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9449   \u001b[39m | \u001b[35m0.05808  \u001b[39m | \u001b[35m0.8662   \u001b[39m | \u001b[35m0.6011   \u001b[39m | \u001b[35m0.7081   \u001b[39m | \u001b[35m0.02058  \u001b[39m | \u001b[35m0.9699   \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9345   \u001b[39m | \u001b[39m0.8324   \u001b[39m | \u001b[39m0.2123   \u001b[39m | \u001b[39m0.1818   \u001b[39m | \u001b[39m0.1834   \u001b[39m | \u001b[39m0.3042   \u001b[39m | \u001b[39m0.5248   \u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.9451   \u001b[39m | \u001b[35m0.4319   \u001b[39m | \u001b[35m0.2912   \u001b[39m | \u001b[35m0.6119   \u001b[39m | \u001b[35m0.1395   \u001b[39m | \u001b[35m0.2921   \u001b[39m | \u001b[35m0.3664   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9337   \u001b[39m | \u001b[39m0.4561   \u001b[39m | \u001b[39m0.7852   \u001b[39m | \u001b[39m0.1997   \u001b[39m | \u001b[39m0.5142   \u001b[39m | \u001b[39m0.5924   \u001b[39m | \u001b[39m0.04645  \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9382   \u001b[39m | \u001b[39m0.7535   \u001b[39m | \u001b[39m0.8575   \u001b[39m | \u001b[39m0.5901   \u001b[39m | \u001b[39m0.3663   \u001b[39m | \u001b[39m0.441    \u001b[39m | \u001b[39m0.7862   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9449   \u001b[39m | \u001b[39m0.1242   \u001b[39m | \u001b[39m0.7127   \u001b[39m | \u001b[39m0.7407   \u001b[39m | \u001b[39m0.1564   \u001b[39m | \u001b[39m0.6338   \u001b[39m | \u001b[39m0.8849   \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9394   \u001b[39m | \u001b[39m0.2104   \u001b[39m | \u001b[39m0.9925   \u001b[39m | \u001b[39m0.2139   \u001b[39m | \u001b[39m0.7011   \u001b[39m | \u001b[39m0.4956   \u001b[39m | \u001b[39m0.2877   \u001b[39m |\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m0.9454   \u001b[39m | \u001b[35m0.7896   \u001b[39m | \u001b[35m0.5355   \u001b[39m | \u001b[35m0.8947   \u001b[39m | \u001b[35m0.4574   \u001b[39m | \u001b[35m0.4225   \u001b[39m | \u001b[35m0.6538   \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m0.1096   \u001b[39m | \u001b[39m0.7057   \u001b[39m | \u001b[39m0.7193   \u001b[39m | \u001b[39m0.09194  \u001b[39m | \u001b[39m0.5677   \u001b[39m | \u001b[39m0.7826   \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9335   \u001b[39m | \u001b[39m0.7282   \u001b[39m | \u001b[39m0.4367   \u001b[39m | \u001b[39m0.4218   \u001b[39m | \u001b[39m0.03042  \u001b[39m | \u001b[39m0.07563  \u001b[39m | \u001b[39m0.6663   \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m0.9456   \u001b[39m | \u001b[35m0.4049   \u001b[39m | \u001b[35m0.2359   \u001b[39m | \u001b[35m0.9713   \u001b[39m | \u001b[35m0.3197   \u001b[39m | \u001b[35m0.5392   \u001b[39m | \u001b[35m0.5915   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9446   \u001b[39m | \u001b[39m0.8039   \u001b[39m | \u001b[39m0.5297   \u001b[39m | \u001b[39m0.8303   \u001b[39m | \u001b[39m0.4542   \u001b[39m | \u001b[39m0.4183   \u001b[39m | \u001b[39m0.6583   \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.06309  \u001b[39m | \u001b[39m0.3082   \u001b[39m | \u001b[39m0.9263   \u001b[39m | \u001b[39m0.7142   \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m0.9234   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9439   \u001b[39m | \u001b[39m0.8755   \u001b[39m | \u001b[39m0.3346   \u001b[39m | \u001b[39m0.336    \u001b[39m | \u001b[39m0.9408   \u001b[39m | \u001b[39m0.5476   \u001b[39m | \u001b[39m0.2241   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9454   \u001b[39m | \u001b[39m0.723    \u001b[39m | \u001b[39m0.09948  \u001b[39m | \u001b[39m0.9322   \u001b[39m | \u001b[39m0.9533   \u001b[39m | \u001b[39m0.4195   \u001b[39m | \u001b[39m0.4777   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.4747   \u001b[39m | \u001b[39m0.4982   \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.9505   \u001b[39m | \u001b[39m0.6936   \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9313   \u001b[39m | \u001b[39m0.3473   \u001b[39m | \u001b[39m0.6776   \u001b[39m | \u001b[39m0.4873   \u001b[39m | \u001b[39m0.09166  \u001b[39m | \u001b[39m0.01519  \u001b[39m | \u001b[39m0.2208   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9441   \u001b[39m | \u001b[39m0.01088  \u001b[39m | \u001b[39m0.002149 \u001b[39m | \u001b[39m0.5904   \u001b[39m | \u001b[39m0.4714   \u001b[39m | \u001b[39m0.6176   \u001b[39m | \u001b[39m0.3002   \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m0.9458   \u001b[39m | \u001b[35m0.8827   \u001b[39m | \u001b[35m0.01677  \u001b[39m | \u001b[35m0.9557   \u001b[39m | \u001b[35m0.2321   \u001b[39m | \u001b[35m0.5543   \u001b[39m | \u001b[35m0.1069   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.1233   \u001b[39m | \u001b[39m0.6828   \u001b[39m | \u001b[39m0.7993   \u001b[39m | \u001b[39m0.1988   \u001b[39m | \u001b[39m0.6764   \u001b[39m | \u001b[39m0.8788   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.9458   \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.544    \u001b[39m | \u001b[39m1.0      \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.9434   \u001b[39m | \u001b[39m0.006497 \u001b[39m | \u001b[39m0.001336 \u001b[39m | \u001b[39m0.9983   \u001b[39m | \u001b[39m0.03946  \u001b[39m | \u001b[39m0.1892   \u001b[39m | \u001b[39m0.2084   \u001b[39m |\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.9461   \u001b[39m | \u001b[35m0.8863   \u001b[39m | \u001b[35m0.06383  \u001b[39m | \u001b[35m0.9428   \u001b[39m | \u001b[35m0.2489   \u001b[39m | \u001b[35m0.5357   \u001b[39m | \u001b[35m0.1752   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.9426   \u001b[39m | \u001b[39m0.06573  \u001b[39m | \u001b[39m0.3433   \u001b[39m | \u001b[39m0.2154   \u001b[39m | \u001b[39m0.9548   \u001b[39m | \u001b[39m0.5764   \u001b[39m | \u001b[39m0.9355   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.9017   \u001b[39m | \u001b[39m0.489    \u001b[39m | \u001b[39m0.9624   \u001b[39m | \u001b[39m0.8222   \u001b[39m | \u001b[39m0.9151   \u001b[39m | \u001b[39m0.2436   \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m0.9807   \u001b[39m | \u001b[39m0.5895   \u001b[39m | \u001b[39m0.9448   \u001b[39m | \u001b[39m0.9823   \u001b[39m | \u001b[39m0.0009913\u001b[39m | \u001b[39m0.8594   \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.9956   \u001b[39m | \u001b[39m0.0708   \u001b[39m | \u001b[39m0.9396   \u001b[39m | \u001b[39m0.8439   \u001b[39m | \u001b[39m0.9614   \u001b[39m | \u001b[39m0.8786   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.9454   \u001b[39m | \u001b[39m0.1735   \u001b[39m | \u001b[39m0.9039   \u001b[39m | \u001b[39m0.978    \u001b[39m | \u001b[39m0.9851   \u001b[39m | \u001b[39m0.9906   \u001b[39m | \u001b[39m0.4568   \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.9458   \u001b[39m | \u001b[39m0.7458   \u001b[39m | \u001b[39m0.004168 \u001b[39m | \u001b[39m0.7981   \u001b[39m | \u001b[39m0.9275   \u001b[39m | \u001b[39m0.9898   \u001b[39m | \u001b[39m0.02472  \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.9458   \u001b[39m | \u001b[39m0.3025   \u001b[39m | \u001b[39m0.05957  \u001b[39m | \u001b[39m0.6957   \u001b[39m | \u001b[39m0.02904  \u001b[39m | \u001b[39m0.9582   \u001b[39m | \u001b[39m0.02963  \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.726    \u001b[39m | \u001b[39m0.03505  \u001b[39m | \u001b[39m0.9275   \u001b[39m | \u001b[39m0.02941  \u001b[39m | \u001b[39m0.9633   \u001b[39m | \u001b[39m0.6169   \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.9446   \u001b[39m | \u001b[39m0.04367  \u001b[39m | \u001b[39m0.09354  \u001b[39m | \u001b[39m0.61     \u001b[39m | \u001b[39m0.01885  \u001b[39m | \u001b[39m0.6649   \u001b[39m | \u001b[39m0.9421   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.9461   \u001b[39m | \u001b[39m0.937    \u001b[39m | \u001b[39m0.9101   \u001b[39m | \u001b[39m0.8876   \u001b[39m | \u001b[39m0.9934   \u001b[39m | \u001b[39m0.6676   \u001b[39m | \u001b[39m0.9852   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.9446   \u001b[39m | \u001b[39m0.038    \u001b[39m | \u001b[39m0.9915   \u001b[39m | \u001b[39m0.4546   \u001b[39m | \u001b[39m0.8825   \u001b[39m | \u001b[39m0.8998   \u001b[39m | \u001b[39m0.8628   \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.9424   \u001b[39m | \u001b[39m0.9696   \u001b[39m | \u001b[39m0.3849   \u001b[39m | \u001b[39m0.9663   \u001b[39m | \u001b[39m0.1036   \u001b[39m | \u001b[39m0.9731   \u001b[39m | \u001b[39m0.1153   \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.9032   \u001b[39m | \u001b[39m0.5602   \u001b[39m | \u001b[39m0.5026   \u001b[39m | \u001b[39m0.9984   \u001b[39m | \u001b[39m0.9711   \u001b[39m | \u001b[39m0.8759   \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m0.9456   \u001b[39m | \u001b[39m0.9655   \u001b[39m | \u001b[39m0.04764  \u001b[39m | \u001b[39m0.957    \u001b[39m | \u001b[39m0.9881   \u001b[39m | \u001b[39m0.07325  \u001b[39m | \u001b[39m0.003209 \u001b[39m |\n",
      "| \u001b[35m39       \u001b[39m | \u001b[35m0.9476   \u001b[39m | \u001b[35m0.0388   \u001b[39m | \u001b[35m0.01585  \u001b[39m | \u001b[35m0.8778   \u001b[39m | \u001b[35m0.9958   \u001b[39m | \u001b[35m0.06103  \u001b[39m | \u001b[35m0.1151   \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.9446   \u001b[39m | \u001b[39m0.1225   \u001b[39m | \u001b[39m0.005746 \u001b[39m | \u001b[39m0.8886   \u001b[39m | \u001b[39m0.9881   \u001b[39m | \u001b[39m0.9226   \u001b[39m | \u001b[39m0.1979   \u001b[39m |\n",
      "| \u001b[39m41       \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.4838   \u001b[39m | \u001b[39m0.0227   \u001b[39m | \u001b[39m0.9185   \u001b[39m | \u001b[39m0.5302   \u001b[39m | \u001b[39m0.03474  \u001b[39m | \u001b[39m0.3343   \u001b[39m |\n",
      "| \u001b[39m42       \u001b[39m | \u001b[39m0.9431   \u001b[39m | \u001b[39m0.2285   \u001b[39m | \u001b[39m0.01096  \u001b[39m | \u001b[39m0.1031   \u001b[39m | \u001b[39m0.9562   \u001b[39m | \u001b[39m0.0371   \u001b[39m | \u001b[39m0.08632  \u001b[39m |\n",
      "| \u001b[39m43       \u001b[39m | \u001b[39m0.9463   \u001b[39m | \u001b[39m0.0171   \u001b[39m | \u001b[39m0.3667   \u001b[39m | \u001b[39m0.994    \u001b[39m | \u001b[39m0.9354   \u001b[39m | \u001b[39m0.2505   \u001b[39m | \u001b[39m0.1893   \u001b[39m |\n",
      "| \u001b[39m44       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m0.09528  \u001b[39m | \u001b[39m0.9124   \u001b[39m | \u001b[39m0.9952   \u001b[39m | \u001b[39m0.9994   \u001b[39m | \u001b[39m0.018    \u001b[39m | \u001b[39m0.8032   \u001b[39m |\n",
      "| \u001b[39m45       \u001b[39m | \u001b[39m0.9458   \u001b[39m | \u001b[39m0.9423   \u001b[39m | \u001b[39m0.6813   \u001b[39m | \u001b[39m0.9648   \u001b[39m | \u001b[39m0.583    \u001b[39m | \u001b[39m0.9863   \u001b[39m | \u001b[39m0.7479   \u001b[39m |\n",
      "| \u001b[39m46       \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.9646   \u001b[39m | \u001b[39m0.4958   \u001b[39m | \u001b[39m0.997    \u001b[39m | \u001b[39m0.9671   \u001b[39m | \u001b[39m0.7684   \u001b[39m | \u001b[39m0.7374   \u001b[39m |\n",
      "| \u001b[39m47       \u001b[39m | \u001b[39m0.9458   \u001b[39m | \u001b[39m0.4445   \u001b[39m | \u001b[39m0.05525  \u001b[39m | \u001b[39m0.9865   \u001b[39m | \u001b[39m0.9655   \u001b[39m | \u001b[39m0.3503   \u001b[39m | \u001b[39m0.04524  \u001b[39m |\n",
      "| \u001b[39m48       \u001b[39m | \u001b[39m0.9429   \u001b[39m | \u001b[39m0.6628   \u001b[39m | \u001b[39m0.02031  \u001b[39m | \u001b[39m0.3254   \u001b[39m | \u001b[39m0.9814   \u001b[39m | \u001b[39m0.9865   \u001b[39m | \u001b[39m0.9344   \u001b[39m |\n",
      "| \u001b[39m49       \u001b[39m | \u001b[39m0.9451   \u001b[39m | \u001b[39m0.2176   \u001b[39m | \u001b[39m0.09026  \u001b[39m | \u001b[39m0.7498   \u001b[39m | \u001b[39m0.9849   \u001b[39m | \u001b[39m0.0219   \u001b[39m | \u001b[39m0.6078   \u001b[39m |\n",
      "| \u001b[39m50       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m0.9899   \u001b[39m | \u001b[39m0.004861 \u001b[39m | \u001b[39m0.9569   \u001b[39m | \u001b[39m0.3619   \u001b[39m | \u001b[39m0.3319   \u001b[39m | \u001b[39m0.8756   \u001b[39m |\n",
      "| \u001b[39m51       \u001b[39m | \u001b[39m0.9446   \u001b[39m | \u001b[39m0.4909   \u001b[39m | \u001b[39m0.02435  \u001b[39m | \u001b[39m0.9094   \u001b[39m | \u001b[39m0.03577  \u001b[39m | \u001b[39m0.4172   \u001b[39m | \u001b[39m0.2128   \u001b[39m |\n",
      "| \u001b[39m52       \u001b[39m | \u001b[39m0.9449   \u001b[39m | \u001b[39m0.9852   \u001b[39m | \u001b[39m0.1408   \u001b[39m | \u001b[39m0.9517   \u001b[39m | \u001b[39m0.8176   \u001b[39m | \u001b[39m0.5509   \u001b[39m | \u001b[39m0.01222  \u001b[39m |\n",
      "| \u001b[39m53       \u001b[39m | \u001b[39m0.9449   \u001b[39m | \u001b[39m0.8482   \u001b[39m | \u001b[39m0.002951 \u001b[39m | \u001b[39m0.222    \u001b[39m | \u001b[39m0.9858   \u001b[39m | \u001b[39m0.9371   \u001b[39m | \u001b[39m0.06644  \u001b[39m |\n",
      "| \u001b[39m54       \u001b[39m | \u001b[39m0.9454   \u001b[39m | \u001b[39m0.6425   \u001b[39m | \u001b[39m0.9636   \u001b[39m | \u001b[39m0.8726   \u001b[39m | \u001b[39m0.9215   \u001b[39m | \u001b[39m0.9754   \u001b[39m | \u001b[39m0.9184   \u001b[39m |\n",
      "| \u001b[39m55       \u001b[39m | \u001b[39m0.9444   \u001b[39m | \u001b[39m0.4772   \u001b[39m | \u001b[39m0.605    \u001b[39m | \u001b[39m0.9297   \u001b[39m | \u001b[39m0.9091   \u001b[39m | \u001b[39m0.7368   \u001b[39m | \u001b[39m0.2277   \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "✅ Best Bayesian Weights Found: [0.01843675 0.00753298 0.41712666 0.47320856 0.02900162 0.05469343]\n",
      "✅ Best Bayesian Ensemble Accuracy: 0.9476\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      2022\n",
      "           1       0.93      0.97      0.95      2022\n",
      "\n",
      "    accuracy                           0.95      4044\n",
      "   macro avg       0.95      0.95      0.95      4044\n",
      "weighted avg       0.95      0.95      0.95      4044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- 1. Dataset Class ---\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 2. Load Individual Model ---\n",
    "def load_model(model_path):\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# --- 3. Weighted Voting Function ---\n",
    "def weighted_ensemble_voting(models, val_loader, weights):\n",
    "    all_logits = []\n",
    "\n",
    "    for model in models:\n",
    "        model_logits = []\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                model_logits.append(outputs.logits.cpu())\n",
    "        \n",
    "        all_logits.append(torch.cat(model_logits, dim=0))\n",
    "    \n",
    "    all_logits = torch.stack(all_logits)  # (n_models, n_samples, n_classes)\n",
    "    \n",
    "    weighted_logits = torch.zeros_like(all_logits[0])\n",
    "    for i, weight in enumerate(weights):\n",
    "        weighted_logits += weight * all_logits[i]\n",
    "    \n",
    "    final_preds = torch.argmax(weighted_logits, dim=1).numpy()\n",
    "    return final_preds\n",
    "\n",
    "# --- 4. Bayesian Optimization for Best Weights ---\n",
    "def bayesian_search_weights(models, val_loader, y_true, init_points=5, n_iter=50):\n",
    "    def black_box_function(**kwargs):\n",
    "        weights = np.array([kwargs[f'w{i}'] for i in range(len(models))])\n",
    "        weights = weights / weights.sum()  # Normalize\n",
    "        preds = weighted_ensemble_voting(models, val_loader, weights)\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        return acc\n",
    "\n",
    "    pbounds = {f'w{i}': (0, 1) for i in range(len(models))}\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=black_box_function,\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(\n",
    "        init_points=init_points,\n",
    "        n_iter=n_iter\n",
    "    )\n",
    "\n",
    "    best_weights = np.array([optimizer.max['params'][f'w{i}'] for i in range(len(models))])\n",
    "    best_weights /= best_weights.sum()  # Normalize\n",
    "    best_accuracy = optimizer.max['target']\n",
    "\n",
    "    return best_weights, best_accuracy\n",
    "\n",
    "# --- 5. Main Function ---\n",
    "def main():\n",
    "    # Load validation data\n",
    "    df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df.to_csv('Preprocessed Fake Reviews Detection Dataset (1).xls')\n",
    "    df['text_'] = df['text_'].fillna('')\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text_'], df['label'], \n",
    "        test_size=0.1, random_state=42, stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length=64)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "    # Load optimized models\n",
    "    abc_model = load_model('abc_optimized_roberta.pt')\n",
    "    aco_model = load_model('aco_optimized_roberta.pt')\n",
    "    bayesian_model = load_model('bayesian_optimized_roberta.pt')\n",
    "    firefly_model = load_model('firefly_optimized_roberta.pt')\n",
    "    gwo_model = load_model('gwo_optimized_roberta.pt')\n",
    "    pso_model = load_model('pso_optimized_roberta.pt')\n",
    "\n",
    "    models = [abc_model, aco_model, bayesian_model, firefly_model, gwo_model, pso_model]\n",
    "    \n",
    "    # --- Search Best Weights ---\n",
    "    print(\"\\n🔍 Bayesian searching for best ensemble weights...\")\n",
    "    weights, best_acc = bayesian_search_weights(models, val_loader, y_val, init_points=5, n_iter=50)\n",
    "\n",
    "    print(f\"\\n✅ Best Bayesian Weights Found: {weights}\")\n",
    "    print(f\"✅ Best Bayesian Ensemble Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # --- Final Prediction ---\n",
    "    final_predictions = weighted_ensemble_voting(models, val_loader, weights)\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_val, final_predictions))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78f61df-d07b-4b8e-8471-2afc115318a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0e8131c-ee2a-499e-abea-26d3e4e018e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble components saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# 1. Define the function to save all components\n",
    "def save_ensemble_components(models, weights, tokenizer, max_length=64):\n",
    "    \"\"\"Save all components needed for the ensemble\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    Path(\"saved_ensemble\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save individual models\n",
    "    model_names = ['abc', 'aco', 'bayesian', 'firefly', 'gwo', 'pso']\n",
    "    for i, model in enumerate(models):\n",
    "        torch.save(model.state_dict(), f\"saved_ensemble/{model_names[i]}_model.pt\")\n",
    "    \n",
    "    # Save weights and configuration\n",
    "    torch.save({\n",
    "        'weights': weights,\n",
    "        'max_length': max_length\n",
    "    }, \"saved_ensemble/ensemble_config.pt\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(\"saved_ensemble/tokenizer\")\n",
    "    \n",
    "    print(\"✅ Ensemble components saved successfully!\")\n",
    "\n",
    "# 2. Load your trained models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Helper function to load individual models\"\"\"\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Load all models (update paths if needed)\n",
    "abc_model = load_model('abc_optimized_roberta.pt')\n",
    "aco_model = load_model('aco_optimized_roberta.pt')\n",
    "bayesian_model = load_model('bayesian_optimized_roberta.pt')\n",
    "firefly_model = load_model('firefly_optimized_roberta.pt')\n",
    "gwo_model = load_model('gwo_optimized_roberta.pt')\n",
    "pso_model = load_model('pso_optimized_roberta.pt')\n",
    "\n",
    "models = [abc_model, aco_model, bayesian_model, firefly_model, gwo_model, pso_model]\n",
    "\n",
    "# 3. Load your optimal weights (replace with your actual weights)\n",
    "weights =[0.01843675, 0.00753298, 0.41712666, 0.47320856, 0.02900162, 0.05469343]  # Example - use your Bayesian-optimized weights\n",
    "\n",
    "# 4. Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 5. Save everything\n",
    "save_ensemble_components(models, weights, tokenizer, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9de9194-98f8-4e95-b523-5097a7c6b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.89 GiB of which 5.75 MiB is free. Process 3950331 has 1.25 GiB memory in use. Process 262053 has 4.80 GiB memory in use. Including non-PyTorch memory, this process has 5.83 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 378.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Initialize detector\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     detector \u001b[38;5;241m=\u001b[39m \u001b[43mFakeReviewDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Test prediction\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     test_review \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLove this!  Well made, sturdy, and very comfortable.  I love it!Very pretty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[37], line 24\u001b[0m, in \u001b[0;36mFakeReviewDetector.__init__\u001b[0;34m(self, ensemble_dir)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[1;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m RobertaForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mensemble_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     ):\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    514\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:1928\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1928\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:1900\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1895\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1900\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1901\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1902\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1903\u001b[0m )\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1906\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:1806\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:693\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/serialization.py:632\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    631\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/storage.py:292\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    291\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_utils.py:99\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse\n\u001b[1;32m     98\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.89 GiB of which 5.75 MiB is free. Process 3950331 has 1.25 GiB memory in use. Process 262053 has 4.80 GiB memory in use. Including non-PyTorch memory, this process has 5.83 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 378.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "class FakeReviewDetector:\n",
    "    def __init__(self, ensemble_dir=\"saved_ensemble\"):\n",
    "        \"\"\"Initialize the detector with saved components\"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load configuration and weights\n",
    "        config = torch.load(f\"{ensemble_dir}/ensemble_config.pt\")\n",
    "        self.weights = config['weights']\n",
    "        self.max_length = config['max_length']\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(f\"{ensemble_dir}/tokenizer\")\n",
    "        \n",
    "        # Load all individual models\n",
    "        self.models = []\n",
    "        model_names = ['abc', 'aco', 'bayesian', 'firefly', 'gwo', 'pso']\n",
    "        \n",
    "        for name in model_names:\n",
    "            model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "            model.load_state_dict(torch.load(f\"{ensemble_dir}/{name}_model.pt\", map_location=self.device))\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Make a prediction on a single review text\"\"\"\n",
    "        # Tokenize input\n",
    "        encoding = self.tokenizer(\n",
    "            str(text),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        with torch.no_grad():\n",
    "            all_logits = []\n",
    "            for model in self.models:\n",
    "                outputs = model(**encoding)\n",
    "                all_logits.append(outputs.logits.cpu())\n",
    "            \n",
    "            # Apply weighted voting\n",
    "            weighted_logits = torch.zeros_like(all_logits[0])\n",
    "            for i, weight in enumerate(self.weights):\n",
    "                weighted_logits += weight * all_logits[i]\n",
    "            \n",
    "            # Get final prediction (0 = Real, 1 = Fake)\n",
    "            pred = torch.argmax(weighted_logits, dim=1).item()\n",
    "        \n",
    "        return \"Fake (CG)\" if pred == 1 else \"Real (OR)\"\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    detector = FakeReviewDetector()\n",
    "    \n",
    "    # Test prediction\n",
    "    test_review = \"Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty.\"\n",
    "    print(f\"Review: {test_review}\")\n",
    "    print(f\"Prediction: {detector.predict(test_review)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a05da1-fcd7-48d3-b418-78c007e66b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                     2.6.0\n",
      "torchaudio                2.6.0\n",
      "torchvision               0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!python -m pip list | findstr torch\n",
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a853fc02-939c-4829-b64e-195201600531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27a9cca6614593a7ca9a271abd9208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully on CPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Force CPU-only mode\n",
    "device = torch.device('cpu')\n",
    "\n",
    "try:\n",
    "    # 1. First load the config\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        num_labels=2,\n",
    "        local_files_only=True  # Use cached version\n",
    "    )\n",
    "    \n",
    "    # 2. Then load weights\n",
    "    model.load_state_dict(\n",
    "        torch.load('best_model.pt', map_location=device),\n",
    "        strict=False  # Helps if architecture slightly differs\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"✅ Model loaded successfully on CPU!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Verify 'best_model.pt' exists in current directory\")\n",
    "    print(\"2. Try absolute path: r'C:\\\\full\\\\path\\\\to\\\\best_model.pt'\")\n",
    "    print(\"3. Check file integrity (size should be >100MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09795f5-7f09-49de-88c1-ac43518448c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nRobertaForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1. Load your saved model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1841\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1841\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\import_utils.py:1829\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1827\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nRobertaForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Load your saved model\n",
    "model_path = 'best_model.pt'\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa8e107-0bd7-40ef-8de4-6e4fa4131874",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Initialize tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RobertaTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa71657-fdaf-4475-9eec-85ea94156644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts, return_probs=False):\n",
    "    \"\"\"\n",
    "    Make predictions on new text inputs\n",
    "    \n",
    "    Args:\n",
    "        texts (str or list): Input text or list of texts\n",
    "        return_probs (bool): Whether to return probability scores\n",
    "        \n",
    "    Returns:\n",
    "        If return_probs=False: Array of predictions (0=real, 1=fake)\n",
    "        If return_probs=True: Tuple of (predictions, probability_scores)\n",
    "    \"\"\"\n",
    "    # Convert single string to list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return preds\n",
    "    \n",
    "    # return (preds, probs) if return_probs else preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5053e43-cf6f-4ac1-b305-08d9506fb2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Example Usage:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Single prediction\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct is fine, wish it had an option to have a separate handle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mprediction[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Batch prediction with confidence scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(texts, return_probs)\u001b[0m\n\u001b[0;32m     15\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [texts]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Tokenize inputs\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\n\u001b[0;32m     19\u001b[0m     texts,\n\u001b[0;32m     20\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m     23\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. Example Usage:\n",
    "\n",
    "# Single prediction\n",
    "sample_text = \"Product is fine, wish it had an option to have a separate handle.\"\n",
    "prediction = predict(sample_text)\n",
    "print(f\"\\nPrediction: {'Fake' if prediction[0] == 1 else 'Real'}\")\n",
    "\n",
    "# Batch prediction with confidence scores\n",
    "sample_texts = [\n",
    "    \"Product is fine, wish it had an option to have a separate handle.\",\n",
    "    \"It is nice bowl and have had a fast shipping!\",\n",
    "    \"Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty\",\n",
    "    \"These are so nice, sturdy, like the color choices too.\"\n",
    "]\n",
    "\n",
    "preds, probs = predict(sample_texts, return_probs=True)\n",
    "\n",
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'text': sample_texts,\n",
    "    'prediction': ['Fake' if p == 1 else 'Real' for p in preds],\n",
    "    'confidence': [f\"{max(prob)*100:.1f}%\" for prob in probs],\n",
    "    'real_prob': [f\"{prob[0]*100:.1f}%\" for prob in probs],\n",
    "    'fake_prob': [f\"{prob[1]*100:.1f}%\" for prob in probs]\n",
    "})\n",
    "\n",
    "print(\"\\nDetailed Predictions:\")\n",
    "print(results.to_string(index=False))  # Using to_string() instead\n",
    "\n",
    "# 4. Evaluate on Test Set\n",
    "if 'X_test' in locals() and 'y_test' in locals():\n",
    "    test_preds = predict(X_test.tolist())\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, test_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Real', 'Fake'], \n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "# else:\n",
    "#     print(\"\\nNote: Test data not available - skipping evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166057e1-5723-462d-a9e3-38ea4a71253d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
